{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed57215f",
   "metadata": {
    "papermill": {
     "duration": 0.005976,
     "end_time": "2025-06-29T17:22:27.898515",
     "exception": false,
     "start_time": "2025-06-29T17:22:27.892539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Quality checks\n",
    "- Check fo missing data or gaps in time series \n",
    "- Check Outliers (if applicable)\n",
    "- Schema validation - already done during ingestion phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afa38cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:27.943765Z",
     "iopub.status.busy": "2025-06-29T17:22:27.943440Z",
     "iopub.status.idle": "2025-06-29T17:22:28.110514Z",
     "shell.execute_reply": "2025-06-29T17:22:28.110174Z"
    },
    "papermill": {
     "duration": 0.187732,
     "end_time": "2025-06-29T17:22:28.111660",
     "exception": false,
     "start_time": "2025-06-29T17:22:27.923928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, expr, to_date, count, mean, stddev, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161363d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:28.128307Z",
     "iopub.status.busy": "2025-06-29T17:22:28.128120Z",
     "iopub.status.idle": "2025-06-29T17:22:28.131313Z",
     "shell.execute_reply": "2025-06-29T17:22:28.130945Z"
    },
    "papermill": {
     "duration": 0.015898,
     "end_time": "2025-06-29T17:22:28.132161",
     "exception": false,
     "start_time": "2025-06-29T17:22:28.116263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "# Get Jar path needed for spark session\n",
    "# For simplicity using locally downloaded jars for delta format\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    proj_dir = os.path.abspath(\"..\")\n",
    "else:\n",
    "    proj_dir = cwd\n",
    "jar_dir = os.path.join(proj_dir, \"jars\")\n",
    "jar1 = os.path.join(jar_dir, \"delta-spark_2.13-4.0.0.jar\")\n",
    "jar2 = os.path.join(jar_dir, \"delta-storage-4.0.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac415a82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:28.144603Z",
     "iopub.status.busy": "2025-06-29T17:22:28.144358Z",
     "iopub.status.idle": "2025-06-29T17:22:31.401926Z",
     "shell.execute_reply": "2025-06-29T17:22:31.401602Z"
    },
    "papermill": {
     "duration": 3.266226,
     "end_time": "2025-06-29T17:22:31.402954",
     "exception": false,
     "start_time": "2025-06-29T17:22:28.136728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 19:22:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"EnergyUseCase\") \\\n",
    "            .config(\"spark.jars\", f\"{jar1},{jar2}\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a7a8a",
   "metadata": {
    "papermill": {
     "duration": 0.013763,
     "end_time": "2025-06-29T17:22:31.418812",
     "exception": false,
     "start_time": "2025-06-29T17:22:31.405049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Checking DQ of public power data .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5412a583",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:31.434837Z",
     "iopub.status.busy": "2025-06-29T17:22:31.434647Z",
     "iopub.status.idle": "2025-06-29T17:22:32.183441Z",
     "shell.execute_reply": "2025-06-29T17:22:32.183197Z"
    },
    "papermill": {
     "duration": 0.754229,
     "end_time": "2025-06-29T17:22:32.184336",
     "exception": false,
     "start_time": "2025-06-29T17:22:31.430107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "power_data = spark.read.format(\"delta\").load(f\"{proj_dir}/data/silver/public_power_data\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d291bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:32.194872Z",
     "iopub.status.busy": "2025-06-29T17:22:32.194746Z",
     "iopub.status.idle": "2025-06-29T17:22:32.198585Z",
     "shell.execute_reply": "2025-06-29T17:22:32.198268Z"
    },
    "papermill": {
     "duration": 0.008492,
     "end_time": "2025-06-29T17:22:32.199406",
     "exception": false,
     "start_time": "2025-06-29T17:22:32.190914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DQ functions \n",
    "def check_for_gaps():\n",
    "    \"\"\" Records per day should have 96 distinct values because of 15 mins interval\"\"\"\n",
    "\n",
    "    res = power_data.withColumn(\"day\", to_date(\"timestamp\")) \\\n",
    "        .groupBy(\"production_type\",\"day\") \\\n",
    "        .agg(count(\"*\").alias(\"records_per_day\")) \\\n",
    "        .orderBy(\"production_type\", \"day\") \n",
    "    cnt = res.select(\"records_per_day\").distinct().count()\n",
    "    invalid_days = res.filter(col(\"records_per_day\") != 96).select(\"day\").distinct().collect()\n",
    "    invalid_days_dict = [row.asDict() for row in invalid_days]\n",
    "    return {\"check\": f\"Data Gaps\", \"status\": \"Pass\" if cnt == 1 else \"Fail\", \"failed_records\": f\"failed on days {invalid_days_dict}\"}\n",
    "\n",
    "def check_missing_field(id_column):\n",
    "    missing_id_count = power_data.filter(col(id_column).isNull()).count()\n",
    "    return {\"check\": f\"{id_column} not null\", \"status\": \"Pass\" if missing_id_count == 0 else \"Fail\", \"failed_records\": missing_id_count}\n",
    "\n",
    "\n",
    "def get_outliers():\n",
    "    \n",
    "    stats = power_data.groupBy(\"production_type\").agg(\n",
    "        mean(\"net_power_produced\").alias(\"mean\"),\n",
    "        stddev(\"net_power_produced\").alias(\"std\")\n",
    "    )\n",
    "    df_with_stats = power_data.join(stats, on=\"production_type\", how=\"left\")\n",
    "    outliers = df_with_stats.filter(\n",
    "        (col(\"net_power_produced\") > col(\"mean\") + 3 * col(\"std\")) |\n",
    "        (col(\"net_power_produced\") < col(\"mean\") - 3 * col(\"std\"))\n",
    "    )\n",
    "    summary= outliers.groupBy(\"production_type\").count()\n",
    "    outlier_info = {row['production_type']: row['count'] for row in summary.collect()}\n",
    "    return {\"check\": f\"Data Outliers\", \"status\": \"Pass\" if bool(outlier_info) == False else \"Fail\", \"failed_records\": f\"Have outliers {outlier_info}\"}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4835368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:32.206842Z",
     "iopub.status.busy": "2025-06-29T17:22:32.206639Z",
     "iopub.status.idle": "2025-06-29T17:22:32.208820Z",
     "shell.execute_reply": "2025-06-29T17:22:32.208577Z"
    },
    "papermill": {
     "duration": 0.006494,
     "end_time": "2025-06-29T17:22:32.209525",
     "exception": false,
     "start_time": "2025-06-29T17:22:32.203031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_data_quality_checks():\n",
    "    results = []\n",
    "\n",
    "    results.append(check_missing_field(\"production_type\"))\n",
    "    results.append(check_for_gaps())\n",
    "    results.append(get_outliers())\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17df4c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-29T17:22:32.221482Z",
     "iopub.status.busy": "2025-06-29T17:22:32.221357Z",
     "iopub.status.idle": "2025-06-29T17:22:40.321812Z",
     "shell.execute_reply": "2025-06-29T17:22:40.321534Z"
    },
    "papermill": {
     "duration": 8.105054,
     "end_time": "2025-06-29T17:22:40.322573",
     "exception": false,
     "start_time": "2025-06-29T17:22:32.217519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 19:22:33 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:==========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                        (0 + 10) / 50]\r",
      "\r",
      "[Stage 2:===============>                                        (14 + 10) / 50]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:=======================================================> (49 + 1) / 50]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:=======================================>                 (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:=======================================>                 (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Quality Report"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>status</th>\n",
       "      <th>failed_records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>production_type not null</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Gaps</td>\n",
       "      <td>Fail</td>\n",
       "      <td>failed on days [{'day': datetime.date(2025, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Outliers</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Have outliers {'Fossil gas': 7, 'Hydro Run-of-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      check status  \\\n",
       "0  production_type not null   Pass   \n",
       "1                 Data Gaps   Fail   \n",
       "2             Data Outliers   Fail   \n",
       "\n",
       "                                      failed_records  \n",
       "0                                                  0  \n",
       "1  failed on days [{'day': datetime.date(2025, 6,...  \n",
       "2  Have outliers {'Fossil gas': 7, 'Hydro Run-of-...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "dq_results = run_data_quality_checks()\n",
    "dq_report_df = pd.DataFrame(dq_results)\n",
    "display(Markdown(\"### Data Quality Report\"))\n",
    "display(dq_report_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.512513,
   "end_time": "2025-06-29T17:22:42.061130",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/zodenath/Desktop/projects/energy-proj/notebooks/data_quality.ipynb",
   "output_path": "/Users/zodenath/Desktop/projects/energy-proj/output/out_DQ_2025-06-29.ipynb",
   "parameters": {},
   "start_time": "2025-06-29T17:22:26.548617",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}