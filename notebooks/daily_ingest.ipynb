{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "hours = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, posexplode, col, size, from_unixtime, to_date, concat_ws, lit, sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, ArrayType, LongType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dotenv()\n",
    "#os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Jar path needed for spark session\n",
    "# For simplicity using locally downloaded jars for delta format\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    proj_dir = os.path.abspath(\"..\")\n",
    "else:\n",
    "    proj_dir = cwd\n",
    "jar_dir = os.path.join(proj_dir, \"jars\")\n",
    "jar1 = os.path.join(jar_dir, \"delta-spark_2.13-4.0.0.jar\")\n",
    "jar2 = os.path.join(jar_dir, \"delta-storage-4.0.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/07/02 09:17:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/02 09:17:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"EnergyUseCase\") \\\n",
    "            .config(\"spark.jars\", f\"{jar1},{jar2}\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "BASE_URL = \"https://api.energy-charts.info\"\n",
    "ENDPOINT_CONFIG = {\n",
    "    \"public_power\": {\n",
    "        \"path\": \"/public_power\",\n",
    "        \"params\": [\"country\", \"start\", \"end\"]\n",
    "    },\n",
    "    \"installed_power\": {\n",
    "        \"path\": \"/installed_power\",\n",
    "        \"params\": [\"country\", \"time_step\", \"installation_decommission\"]\n",
    "    },\n",
    "    \"price\": {\n",
    "        \"path\": \"/price\",\n",
    "        \"params\": [\"bzn\", \"start\", \"end\"]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Parameters\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "back_utc = now_utc - timedelta(hours=hours)  # <- set it here when running notebook locally\n",
    "start_time = back_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "end_time = now_utc.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# For Testing locally fetched 2 months data\n",
    "#start_time = \"2025-04-01 00:00\"\n",
    "#end_time = \"2025-05-30 23:59\"\n",
    "country = \"de\"\n",
    "bidding_zone = \"DE-LU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_api_data(endpoint_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the energy charts \n",
    "    \"\"\"\n",
    "\n",
    "    if endpoint_name not in ENDPOINT_CONFIG:\n",
    "        raise ValueError(f\"Unsupported endpoint: {endpoint_name}\")\n",
    "    \n",
    "    config = ENDPOINT_CONFIG[endpoint_name]\n",
    "    path = config[\"path\"]\n",
    "    required_params = config[\"params\"]\n",
    "    missing = [p for p in required_params if p not in kwargs]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required params: {missing} for endpoint '{endpoint_name}'\")\n",
    "    params = {k: v for k, v in kwargs.items() if k in required_params}\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    try:\n",
    "        print(f\"Fetching data from api with these params - {params}\")\n",
    "        response = requests.get(url, params=params, verify=False)\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        data = response.json() \n",
    "        if not data:\n",
    "            print(\"Empty response received.\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get public power data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from api with these params - {'country': 'de', 'start': '2025-07-01 07:18:09', 'end': '2025-07-02 07:18:09'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/energy/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.energy-charts.info'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        unix_seconds|    production_types|deprecated|\n",
      "+--------------------+--------------------+----------+\n",
      "|[1751347800, 1751...|[{Hydro pumped st...|     false|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch public power\n",
    "public_power = fetch_api_data(\"public_power\",\n",
    "                            country=country,\n",
    "                            start=start_time,\n",
    "                            end=end_time\n",
    "                        )\n",
    "# Create raw dataframe \n",
    "if public_power is not None and isinstance(public_power, dict):\n",
    "    # Define expected schema\n",
    "    public_power_schema = StructType([\n",
    "        StructField(\"unix_seconds\", ArrayType(LongType()), True),\n",
    "        StructField(\"production_types\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"data\", ArrayType(FloatType()), True)\n",
    "            ])\n",
    "        ), True),\n",
    "        StructField(\"deprecated\", BooleanType(), True)\n",
    "    ])\n",
    "    public_power_df = spark.createDataFrame([public_power], schema=public_power_schema)\n",
    "    public_power_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 09:18:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-------------------+\n",
      "|     production_type|net_power_produced|          timestamp|\n",
      "+--------------------+------------------+-------------------+\n",
      "|Hydro pumped stor...|            -545.0|2025-07-02 05:15:00|\n",
      "|             Biomass|            3992.3|2025-07-01 23:30:00|\n",
      "+--------------------+------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "root\n",
      " |-- production_type: string (nullable = true)\n",
      " |-- net_power_produced: float (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "Transform public power data and write to storage \n",
    "\"\"\"\n",
    "public_power_df = public_power_df.withColumn(\"production_type\", explode(\"production_types\"))\n",
    "public_power_df = public_power_df.select(\n",
    "                col(\"unix_seconds\"),\n",
    "                col(\"production_type.name\").alias(\"production_type\"),\n",
    "                posexplode(col(\"production_type.data\")).alias(\"pos\", \"value\")\n",
    "                )\n",
    "# Handle edge cases such as more data points / missing ts for data points\n",
    "public_power_df = public_power_df.filter(col(\"pos\") < size(col(\"unix_seconds\")))\n",
    "public_power_df = public_power_df.withColumn(\"ts\", col(\"unix_seconds\")[col(\"pos\")])\n",
    "public_power_df = public_power_df.select(\"ts\", \n",
    "                                    \"production_type\", \n",
    "                                    col(\"value\").alias(\"net_power_produced\"))\n",
    "public_power_data = public_power_df.withColumn(\"timestamp\", from_unixtime(col(\"ts\")).cast(\"timestamp\")).drop(\"ts\") \n",
    "public_power_data = public_power_data.dropDuplicates() \n",
    "# Write to storage in Delta format\n",
    "public_power_data.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .partitionBy(\"production_type\") \\\n",
    "                .save(f\"{proj_dir}/data/silver/public_power_data\")  \n",
    "\n",
    "public_power_data.show(2)\n",
    "public_power_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/28 20:51:51 WARN TaskSetManager: Stage 47 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          timestamp|\n",
      "+-------------------+\n",
      "|2025-04-01 00:00:00|\n",
      "|2025-04-01 00:15:00|\n",
      "|2025-04-01 00:30:00|\n",
      "|2025-04-01 00:45:00|\n",
      "|2025-04-01 01:00:00|\n",
      "|2025-04-01 01:15:00|\n",
      "|2025-04-01 01:30:00|\n",
      "|2025-04-01 01:45:00|\n",
      "|2025-04-01 02:00:00|\n",
      "|2025-04-01 02:15:00|\n",
      "|2025-04-01 02:30:00|\n",
      "|2025-04-01 02:45:00|\n",
      "|2025-04-01 03:00:00|\n",
      "|2025-04-01 03:15:00|\n",
      "|2025-04-01 03:30:00|\n",
      "|2025-04-01 03:45:00|\n",
      "|2025-04-01 04:00:00|\n",
      "|2025-04-01 04:15:00|\n",
      "|2025-04-01 04:30:00|\n",
      "|2025-04-01 04:45:00|\n",
      "|2025-04-01 05:00:00|\n",
      "|2025-04-01 05:15:00|\n",
      "|2025-04-01 05:30:00|\n",
      "|2025-04-01 05:45:00|\n",
      "|2025-04-01 06:00:00|\n",
      "|2025-04-01 06:15:00|\n",
      "|2025-04-01 06:30:00|\n",
      "|2025-04-01 06:45:00|\n",
      "|2025-04-01 07:00:00|\n",
      "|2025-04-01 07:15:00|\n",
      "|2025-04-01 07:30:00|\n",
      "|2025-04-01 07:45:00|\n",
      "|2025-04-01 08:00:00|\n",
      "|2025-04-01 08:15:00|\n",
      "|2025-04-01 08:30:00|\n",
      "|2025-04-01 08:45:00|\n",
      "|2025-04-01 09:00:00|\n",
      "|2025-04-01 09:15:00|\n",
      "|2025-04-01 09:30:00|\n",
      "|2025-04-01 09:45:00|\n",
      "|2025-04-01 10:00:00|\n",
      "|2025-04-01 10:15:00|\n",
      "|2025-04-01 10:30:00|\n",
      "|2025-04-01 10:45:00|\n",
      "|2025-04-01 11:00:00|\n",
      "|2025-04-01 11:15:00|\n",
      "|2025-04-01 11:30:00|\n",
      "|2025-04-01 11:45:00|\n",
      "|2025-04-01 12:00:00|\n",
      "|2025-04-01 12:15:00|\n",
      "+-------------------+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "#public_power_data.select(\"timestamp\").distinct().sort(\"timestamp\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from api with these params - {'bzn': 'DE-LU', 'start': '2025-06-28 14:30:43', 'end': '2025-06-29 14:30:43'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/energy/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.energy-charts.info'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "+--------------------+--------------------+--------------------+---------+----------+\n",
      "|        license_info|        unix_seconds|               price|     unit|deprecated|\n",
      "+--------------------+--------------------+--------------------+---------+----------+\n",
      "|CC BY 4.0 (creati...|[1751115600, 1751...|[-1.01, -1.0, 2.9...|EUR / MWh|     false|\n",
      "+--------------------+--------------------+--------------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch price data\n",
    "price_data = fetch_api_data(\"price\",\n",
    "                            bzn=bidding_zone,\n",
    "                            start=start_time,\n",
    "                            end=end_time\n",
    "                       )\n",
    "# Create raw dataframe \n",
    "if price_data is not None and isinstance(price_data, dict):\n",
    "    # Define price schema\n",
    "    price_data_schema = StructType([\n",
    "        StructField(\"license_info\", StringType(), True),\n",
    "        StructField(\"unix_seconds\", ArrayType(LongType()), True),\n",
    "        StructField(\"price\", ArrayType(FloatType()), True),\n",
    "        StructField(\"unit\", StringType(), True),\n",
    "        StructField(\"deprecated\", BooleanType(), True)\n",
    "    ])\n",
    "    price_df = spark.createDataFrame([price_data], schema=price_data_schema)\n",
    "    price_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|price|          timestamp|\n",
      "+-----+-------------------+\n",
      "| -5.4|2025-06-29 12:00:00|\n",
      "|95.07|2025-06-29 00:00:00|\n",
      "+-----+-------------------+\n",
      "only showing top 2 rows\n",
      "root\n",
      " |-- price: float (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" Transform price data \"\"\"\n",
    "price_df = price_df.select(\n",
    "                posexplode(\"unix_seconds\").alias(\"pos\", \"unix_ts\"),\n",
    "                col(\"price\")[col(\"pos\")].alias(\"price\")\n",
    "            )\n",
    "price_data = price_df.withColumn(\"timestamp\", from_unixtime(col(\"unix_ts\")).cast(\"timestamp\")).drop(\"unix_ts\", \"pos\")\n",
    "price_data = price_data.dropDuplicates()\n",
    "\n",
    "# Write to storage in Delta format\n",
    "price_data.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .save(f\"{proj_dir}/data/silver/price_data\") \n",
    "\n",
    "price_data.show(2)\n",
    "price_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Data Quality step here to check DQ issues of daily Data\n",
    "- Checking DQ of public power data for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing:  33%|███▎      | 4/12 [00:01<00:01,  4.10cell/s]WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/07/02 09:24:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/02 09:24:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/02 09:24:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "Executing:  67%|██████▋   | 8/12 [00:05<00:02,  1.41cell/s]25/07/02 09:24:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "Executing: 100%|██████████| 12/12 [00:13<00:00,  1.11s/cell]                    \n"
     ]
    }
   ],
   "source": [
    "import papermill as pm\n",
    "try:\n",
    "    pm.execute_notebook(\n",
    "        f\"{proj_dir}/notebooks/data_quality.ipynb\",\n",
    "        f\"{proj_dir}/output/out_DQ_{start_time}.ipynb\",\n",
    "        parameters=dict(start_time=start_time, end_time=end_time)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To handle java.io.IOException: Failed to delete\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
