{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality checks\n",
    "- Check fo missing data or gaps in time series \n",
    "- Check Outliers (if applicable)\n",
    "- Schema validation - already done during ingestion phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "start_time = \"\"\n",
    "end_time = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, expr, to_date, count, mean, stddev, col,to_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dotenv()\n",
    "#os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "# Get Jar path needed for spark session\n",
    "# For simplicity using locally downloaded jars for delta format\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    proj_dir = os.path.abspath(\"..\")\n",
    "else:\n",
    "    proj_dir = cwd\n",
    "jar_dir = os.path.join(proj_dir, \"jars\")\n",
    "jar1 = os.path.join(jar_dir, \"delta-spark_2.13-4.0.0.jar\")\n",
    "jar2 = os.path.join(jar_dir, \"delta-storage-4.0.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"EnergyUseCase\") \\\n",
    "            .config(\"spark.jars\", f\"{jar1},{jar2}\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking DQ of public power data .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "power_data = spark.read.format(\"delta\").load(f\"{proj_dir}/data/silver/public_power_data\")  \n",
    "power_data_24hr = power_data.filter((col(\"timestamp\") >= to_timestamp(lit(start_time))) & (col(\"timestamp\") <= to_timestamp(lit(end_time))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQ functions \n",
    "def check_for_gaps():\n",
    "    \"\"\" Records per day should have 96 distinct values because of 15 mins interval\"\"\"\n",
    "\n",
    "    res = power_data_24hr.withColumn(\"day\", to_date(\"timestamp\")) \\\n",
    "        .groupBy(\"production_type\",\"day\") \\\n",
    "        .agg(count(\"*\").alias(\"records_per_day\")) \\\n",
    "        .orderBy(\"production_type\", \"day\") \n",
    "    cnt = res.select(\"records_per_day\").distinct().count()\n",
    "    print(cnt)\n",
    "    invalid_days = res.filter(col(\"records_per_day\") != 96).select(\"day\").distinct().collect()\n",
    "    print(invalid_days)\n",
    "    invalid_days_dict = [row.asDict() for row in invalid_days]\n",
    "    return {\"check\": f\"Data Gaps\", \"status\": \"Pass\" if cnt == 1 else \"Fail\", \"failed_records\": f\"failed on days {invalid_days_dict}\"}\n",
    "\n",
    "def check_missing_field(id_column):\n",
    "    missing_id_count = power_data_24hr.filter(col(id_column).isNull()).count()\n",
    "    return {\"check\": f\"{id_column} not null\", \"status\": \"Pass\" if missing_id_count == 0 else \"Fail\", \"failed_records\": missing_id_count}\n",
    "\n",
    "\n",
    "def get_outliers():\n",
    "    \n",
    "    stats = power_data_24hr.groupBy(\"production_type\").agg(\n",
    "        mean(\"net_power_produced\").alias(\"mean\"),\n",
    "        stddev(\"net_power_produced\").alias(\"std\")\n",
    "    )\n",
    "    df_with_stats = power_data_24hr.join(stats, on=\"production_type\", how=\"left\")\n",
    "    outliers = df_with_stats.filter(\n",
    "        (col(\"net_power_produced\") > col(\"mean\") + 3 * col(\"std\")) |\n",
    "        (col(\"net_power_produced\") < col(\"mean\") - 3 * col(\"std\"))\n",
    "    )\n",
    "    summary= outliers.groupBy(\"production_type\").count()\n",
    "    outlier_info = {row['production_type']: row['count'] for row in summary.collect()}\n",
    "    return {\"check\": f\"Data Outliers\", \"status\": \"Pass\" if bool(outlier_info) == False else \"Fail\", \"failed_records\": f\"Have outliers {outlier_info}\"}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_data_quality_checks():\n",
    "    results = []\n",
    "\n",
    "    results.append(check_missing_field(\"production_type\"))\n",
    "    results.append(check_for_gaps())\n",
    "    results.append(get_outliers())\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[Row(day=datetime.date(2025, 7, 1))]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Data Quality Report"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>status</th>\n",
       "      <th>failed_records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>production_type not null</td>\n",
       "      <td>Pass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Gaps</td>\n",
       "      <td>Pass</td>\n",
       "      <td>failed on days [{'day': datetime.date(2025, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Outliers</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Have outliers {'Fossil coal-derived gas': 2, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      check status  \\\n",
       "0  production_type not null   Pass   \n",
       "1                 Data Gaps   Pass   \n",
       "2             Data Outliers   Fail   \n",
       "\n",
       "                                      failed_records  \n",
       "0                                                  0  \n",
       "1  failed on days [{'day': datetime.date(2025, 7,...  \n",
       "2  Have outliers {'Fossil coal-derived gas': 2, '...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "dq_results = run_data_quality_checks()\n",
    "dq_report_df = pd.DataFrame(dq_results)\n",
    "display(Markdown(\"### Data Quality Report\"))\n",
    "display(dq_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
