{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample BI queries\n",
    "- Trend of daily public net electricity production in Germany for each production type. \n",
    "- Prediction of underperformance of public net electricity on 30min intervals.\n",
    "- Analysis of daily price against the net power for offshore and onshore wind (= production_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trunc,avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Jar path needed for spark session\n",
    "# For simplicity using locally downloaded jars for delta format\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    proj_dir = os.path.abspath(\"..\")\n",
    "else:\n",
    "    proj_dir = cwd\n",
    "jar_dir = os.path.join(proj_dir, \"jars\")\n",
    "jar1 = os.path.join(jar_dir, \"delta-spark_2.13-4.0.0.jar\")\n",
    "jar2 = os.path.join(jar_dir, \"delta-storage-4.0.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 18:37:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"EnergyBI_Insights\") \\\n",
    "            .config(\"spark.jars\", f\"{jar1},{jar2}\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", f\"{proj_dir}/data-warehouse\") \\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/Users/zodenath/Desktop/projects/energy-proj/data-warehouse'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data to SQL warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" CREATE SCHEMA IF NOT EXISTS energy \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lake_path = f\"{proj_dir}/data/silver/public_power_data\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS energy.public_power_data\n",
    "            USING DELTA\n",
    "            LOCATION '{data_lake_path}'\n",
    "\"\"\")\n",
    "\n",
    "price_lake_path = f\"{proj_dir}/data/silver/price_data\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS energy.price\n",
    "            USING DELTA\n",
    "            LOCATION '{price_lake_path}'\n",
    "\"\"\")\n",
    "\n",
    "installed_lake_path = f\"{proj_dir}/data/silver/public_power_data\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS energy.installed_power_data\n",
    "            USING DELTA\n",
    "            LOCATION '{installed_lake_path}'\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|      date|     production_type|daily_net_production|\n",
      "+----------+--------------------+--------------------+\n",
      "|2025-04-01|             Biomass|   395143.2001953125|\n",
      "|2025-04-01|Cross border elec...|  218034.29940795898|\n",
      "|2025-04-01|Fossil brown coal...|   934869.3037109375|\n",
      "|2025-04-01|Fossil coal-deriv...|   59497.80001831055|\n",
      "|2025-04-01|          Fossil gas|   576568.5981445312|\n",
      "|2025-04-01|    Fossil hard coal|   400229.3992919922|\n",
      "|2025-04-01|          Fossil oil|    32854.1008605957|\n",
      "|2025-04-01|          Geothermal|  2021.1000022888184|\n",
      "|2025-04-01|  Hydro Run-of-River|  163559.99951171875|\n",
      "|2025-04-01|Hydro pumped storage|  112119.10014736652|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Trend of daily public net electricity production in Germany for each production type.\n",
    "\n",
    "daily_trend = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DATE(timestamp) as date,\n",
    "        production_type,\n",
    "        SUM(net_power_produced) AS daily_net_production\n",
    "    FROM energy.public_power_data\n",
    "    GROUP BY DATE(timestamp), production_type\n",
    "    ORDER BY date, production_type\n",
    "\"\"\")\n",
    "daily_trend.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===================>                                      (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------------+\n",
      "|     production_type|     interval_start|       interval_end|       total_power|\n",
      "+--------------------+-------------------+-------------------+------------------+\n",
      "|Fossil brown coal...|2025-04-01 00:00:00|2025-04-01 00:30:00|  23208.2998046875|\n",
      "|               Solar|2025-04-01 00:00:00|2025-04-01 00:30:00|               0.0|\n",
      "|Cross border elec...|2025-04-01 00:00:00|2025-04-01 00:30:00|   9019.2001953125|\n",
      "|       Residual load|2025-04-01 00:00:00|2025-04-01 00:30:00|     79468.8984375|\n",
      "|       Wind offshore|2025-04-01 00:00:00|2025-04-01 00:30:00|1679.7999877929688|\n",
      "|        Wind onshore|2025-04-01 00:00:00|2025-04-01 00:30:00|  13683.7001953125|\n",
      "|Hydro pumped storage|2025-04-01 00:00:00|2025-04-01 00:30:00| 659.3000183105469|\n",
      "|          Fossil gas|2025-04-01 00:00:00|2025-04-01 00:30:00|  15752.2001953125|\n",
      "|               Waste|2025-04-01 00:00:00|2025-04-01 00:30:00|            1781.5|\n",
      "|Fossil coal-deriv...|2025-04-01 00:00:00|2025-04-01 00:30:00|1286.3999633789062|\n",
      "+--------------------+-------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Prediction of underperformance of public net electricity on 30min intervals.\n",
    "unperform_prediction = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        production_type,\n",
    "        window.start AS interval_start,\n",
    "        window.end AS interval_end,\n",
    "        SUM(net_power_produced) AS total_power\n",
    "    FROM (\n",
    "        SELECT *, window(timestamp, '30 minutes') AS window\n",
    "        FROM energy.public_power_data\n",
    "    )\n",
    "    GROUP BY production_type, window\n",
    "    ORDER BY interval_start\n",
    "\"\"\")\n",
    "unperform_prediction.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+-----------------+\n",
      "|      date|production_type|      total_power|        avg_price|\n",
      "+----------+---------------+-----------------+-----------------+\n",
      "|2025-06-28|  Wind offshore|92742.40063476562|68.32333257463243|\n",
      "|2025-06-28|   Wind onshore| 221999.998046875|68.32333257463243|\n",
      "|2025-06-29|  Wind offshore|94731.20101928711|42.32374995946884|\n",
      "|2025-06-29|   Wind onshore|         300561.0|42.32374995946884|\n",
      "+----------+---------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysis of daily price against the net power for offshore and onshore wind (= production_type)\n",
    "\n",
    "price_vs_power = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DATE(p.timestamp) AS date,\n",
    "        p.production_type,\n",
    "        SUM(p.net_power_produced) AS total_power,\n",
    "        AVG(pr.price) AS avg_price\n",
    "    FROM energy.public_power_data p\n",
    "    JOIN energy.price pr\n",
    "        ON p.timestamp = pr.timestamp\n",
    "    WHERE p.production_type IN ('Wind offshore', 'Wind onshore')\n",
    "    GROUP BY DATE(p.timestamp), p.production_type\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "price_vs_power.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ad-hoc analytics without creating tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#public_power = spark.read.format(\"delta\").load(f\"{proj_dir}/data/silver/public_power_data\")  \n",
    "#price = spark.read.format(\"delta\").load(f\"{proj_dir}/data/silver/price_data\")   \n",
    "#installed_power = spark.read.format(\"delta\").load(f\"{proj_dir}/data/silver/installed_power_data\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#public_power.createOrReplaceTempView(\"net_power\")\n",
    "#price.createOrReplaceTempView(\"price\")\n",
    "#installed_power.createOrReplaceTempView(\"installed_power\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
