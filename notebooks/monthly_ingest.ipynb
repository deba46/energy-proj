{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "days = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, posexplode, col, size,to_date, concat_ws, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, BooleanType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_dotenv()\n",
    "#os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Jar path needed for spark session\n",
    "# For simplicity using locally downloaded jars for delta format\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith(\"notebooks\"):\n",
    "    proj_dir = os.path.abspath(\"..\")\n",
    "else:\n",
    "    proj_dir = cwd\n",
    "jar_dir = os.path.join(proj_dir, \"jars\")\n",
    "jar1 = os.path.join(jar_dir, \"delta-spark_2.13-4.0.0.jar\")\n",
    "jar2 = os.path.join(jar_dir, \"delta-storage-4.0.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/06/28 17:05:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/28 17:05:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/28 17:05:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"EnergyUseCase\") \\\n",
    "            .config(\"spark.jars\", f\"{jar1},{jar2}\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "BASE_URL = \"https://api.energy-charts.info\"\n",
    "ENDPOINT_CONFIG = { \n",
    "    \"installed_power\": {\n",
    "        \"path\": \"/installed_power\",\n",
    "        \"params\": [\"country\", \"time_step\", \"installation_decommission\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Parameters\n",
    "country= \"de\"\n",
    "time_step = \"monthly\"\n",
    "#start_time = \"2025-04-01 00:00\"\n",
    "#end_time = \"2025-04-30 00:00\"\n",
    "end_time = datetime.now(timezone.utc)\n",
    "start_time = end_time - timedelta(days=days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_api_data(endpoint_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Fetch JSON data from the energy charts \n",
    "    \"\"\"\n",
    "\n",
    "    if endpoint_name not in ENDPOINT_CONFIG:\n",
    "        raise ValueError(f\"Unsupported endpoint: {endpoint_name}\")\n",
    "    \n",
    "    config = ENDPOINT_CONFIG[endpoint_name]\n",
    "    path = config[\"path\"]\n",
    "    required_params = config[\"params\"]\n",
    "    missing = [p for p in required_params if p not in kwargs]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required params: {missing} for endpoint '{endpoint_name}'\")\n",
    "    params = {k: v for k, v in kwargs.items() if k in required_params}\n",
    "    url = f\"{BASE_URL}{path}\"\n",
    "    try:\n",
    "        print(f\"Fetching data from api with these params - {params}\")\n",
    "        response = requests.get(url, params=params, verify=False)\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        data = response.json() \n",
    "        if not data:\n",
    "            print(\"Empty response received.\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from api with these params - {'country': 'de', 'time_step': 'monthly', 'installation_decommission': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/energy/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.energy-charts.info'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                time|    production_types|deprecated|\n",
      "+--------------------+--------------------+----------+\n",
      "|[01.2002, 02.2002...|[{Biomass, [0.0, ...|     false|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch installed power\n",
    "installed_power = fetch_api_data(\"installed_power\",\n",
    "                            country=country,\n",
    "                            time_step=time_step,  \n",
    "                           installation_decommission=False\n",
    "                        )\n",
    "# Create raw dataframe \n",
    "if installed_power is not None and isinstance(installed_power, dict):\n",
    "    # Define schema\n",
    "    installed_power_schema = StructType([\n",
    "        StructField(\"time\", ArrayType(StringType()), True),\n",
    "        StructField(\"production_types\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"data\", ArrayType(FloatType()), True)\n",
    "            ])\n",
    "        ), True),\n",
    "        StructField(\"deprecated\", BooleanType(), True)\n",
    "    ])\n",
    "    installed_power_df = spark.createDataFrame([installed_power], schema=installed_power_schema)\n",
    "    installed_power_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/28 17:05:49 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+\n",
      "|      date|     production_type|installed_power|\n",
      "+----------+--------------------+---------------+\n",
      "|2025-05-01|Battery Storage (...|         20.537|\n",
      "|2025-05-01|Battery Storage (...|         13.961|\n",
      "|2025-05-01|         Solar gross|        106.399|\n",
      "|2025-05-01|             Biomass|          9.202|\n",
      "|2025-05-01|           Solar net|         94.911|\n",
      "|2025-05-01|       Wind offshore|          9.215|\n",
      "|2025-05-01|        Wind onshore|          65.04|\n",
      "+----------+--------------------+---------------+\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = false)\n",
      " |-- production_type: string (nullable = true)\n",
      " |-- installed_power: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "installed_power_df = installed_power_df.withColumn(\"production_type\", explode(\"production_types\"))\n",
    "installed_power_df = installed_power_df.select(\n",
    "    col(\"time\"),\n",
    "    col(\"production_type.name\").alias(\"production_type\"),\n",
    "    posexplode(col(\"production_type.data\")).alias(\"pos\", \"value\")\n",
    ")\n",
    "installed_power_df = installed_power_df.filter(col(\"pos\") < size(col(\"time\")))\n",
    "installed_power_df = installed_power_df.withColumn(\"monthyear\", col(\"time\")[col(\"pos\")]).drop(\"time\")\n",
    "# Format date filed        \n",
    "installed_power_df = installed_power_df.withColumn(\"date\", to_date(concat_ws(\"-\", lit(\"01\"), col(\"monthyear\")), \"dd-MM.yyyy\") )\n",
    "\n",
    "# Filter dataframe to get required data based on start and end time\n",
    "start_year, start_month = start_time.year, start_time.month\n",
    "end_year, end_month = start_time.year, start_time.month\n",
    "start_filter = to_date(lit(f\"{start_year}-{start_month}-01\"))\n",
    "end_filter = to_date(lit(f\"{end_year}-{end_month}-01\"))\n",
    "if start_year == end_year and start_month == end_month:\n",
    "    filtered = installed_power_df.filter(col(\"date\") == start_filter)\n",
    "else:\n",
    "    filtered = installed_power_df.filter((col(\"date\") >= start_filter) & (col(\"date\") <= end_filter))\n",
    "\n",
    "installed_power_data = filtered.select(\"date\", \n",
    "                                        \"production_type\", \n",
    "                                        col(\"value\").alias(\"installed_power\")).drop(\"monthyear\")\n",
    "installed_power_data = installed_power_data.dropDuplicates() \n",
    "# Write to storage in Delta format\n",
    "installed_power_data.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .option(\"mergeSchema\", \"true\") \\\n",
    "                    .partitionBy(\"date\") \\\n",
    "                    .save(f\"{proj_dir}/data/silver/installed_power_data\")  \n",
    "\n",
    "installed_power_data.show(10)\n",
    "installed_power_data.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
